{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b47b15de-64a5-4fa9-a688-23d3efa9a2f4",
    "_uuid": "0cc385a7-98f6-4883-96eb-7b89c7c9aa1c",
    "papermill": {
     "duration": 0.016533,
     "end_time": "2022-04-12T14:48:23.471825",
     "exception": false,
     "start_time": "2022-04-12T14:48:23.455292",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"width:100%; height:140px\">\n",
    "    <img src=\"https://www.kuleuven.be/internationaal/thinktank/fotos-en-logos/ku-leuven-logo.png/image_preview\" width = 300px, heigh = auto align=left>\n",
    "</div>\n",
    "\n",
    "\n",
    "KUL H02A5a Computer Vision: Group Assignment 2\n",
    "---------------------------------------------------------------\n",
    "Student numbers: <span style=\"color:blue\">r0703889, r0909802, r0716758, r0916443, r0822692</span>. \n",
    "\n",
    "In this group assignment your team will delve into some deep learning applications for computer vision. The assignment will be delivered in the same groups from *Group assignment 1* and you start from this template notebook. The notebook you submit for grading is the last notebook you submit in the [Kaggle competition](https://www.kaggle.com/t/d11be6a431b84198bc85f54ae7e2563f) prior to the deadline on **Tuesday 24 May 23:59**. Closely follow [these instructions](https://github.com/gourie/kaggle_inclass) for joining the competition, sharing your notebook with the TAs and making a valid notebook submission to the competition. A notebook submission not only produces a *submission.csv* file that is used to calculate your competition score, it also runs the entire notebook and saves its output as if it were a report. This way it becomes an all-in-one-place document for the TAs to review. As such, please make sure that your final submission notebook is self-contained and fully documented (e.g. provide strong arguments for the design choices that you make). Most likely, this notebook format is not appropriate to run all your experiments at submission time (e.g. the training of CNNs is a memory hungry and time consuming process; due to limited Kaggle resources). It can be a good idea to distribute your code otherwise and only summarize your findings, together with your final predictions, in the submission notebook. For example, you can substitute experiments with some text and figures that you have produced \"offline\" (e.g. learning curves and results on your internal validation set or even the test set for different architectures, pre-processing pipelines, etc). We advise you to first go through the PDF of this assignment entirely before you really start. Then, it can be a good idea to go through this notebook and use it as your first notebook submission to the competition. You can make use of the *Group assignment 2* forum/discussion board on Toledo if you have any questions. Good luck and have fun!\n",
    "\n",
    "---------------------------------------------------------------\n",
    "NOTES:\n",
    "* This notebook is just a template. Please keep the five main sections, but feel free to adjust further in any way you please!\n",
    "* Clearly indicate the improvements that you make! You can for instance use subsections like: *3.1. Improvement: applying loss function f instead of g*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "35358cfb-b13d-4277-8dd5-4e663c8cd775",
    "_uuid": "3b40b846-d7da-46d8-b354-c6d5c5ded56e",
    "papermill": {
     "duration": 0.014397,
     "end_time": "2022-04-12T14:48:23.501501",
     "exception": false,
     "start_time": "2022-04-12T14:48:23.487104",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Overview\n",
    "This assignment consists of *three main parts* for which we expect you to provide code and extensive documentation in the notebook:\n",
    "* Image classification (Sect. 2)\n",
    "* Semantic segmentation (Sect. 3)\n",
    "* Adversarial attacks (Sect. 4)\n",
    "\n",
    "In the first part, you will train an end-to-end neural network for image classification. In the second part, you will do the same for semantic segmentation. For these two tasks we expect you to put a significant effort into optimizing performance and as such competing with fellow students via the Kaggle competition. In the third part, you will try to find and exploit the weaknesses of your classification and/or segmentation network. For the latter there is no competition format, but we do expect you to put significant effort in achieving good performance on the self-posed goal for that part. Finally, we ask you to reflect and produce an overall discussion with links to the lectures and \"real world\" computer vision (Sect. 5). It is important to note that only a small part of the grade will reflect the actual performance of your networks. However, we do expect all things to work! In general, we will evaluate the correctness of your approach and your understanding of what you have done that you demonstrate in the descriptions and discussions in the final notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014263,
     "end_time": "2022-04-12T14:48:23.530341",
     "exception": false,
     "start_time": "2022-04-12T14:48:23.516078",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.1 Deep learning resources\n",
    "If you did not yet explore this in *Group assignment 1 (Sect. 2)*, we recommend using the TensorFlow and/or Keras library for building deep learning models. You can find a nice crash course [here](https://colab.research.google.com/drive/1UCJt8EYjlzCs1H1d1X0iDGYJsHKwu-NO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "7ddf657a-b938-4a49-87dc-b0db9af9156d",
    "_uuid": "c65ea4f1-cc90-408f-b8e0-7c7399ec7e21",
    "execution": {
     "iopub.execute_input": "2022-05-20T15:12:38.153761Z",
     "iopub.status.busy": "2022-05-20T15:12:38.153091Z",
     "iopub.status.idle": "2022-05-20T15:12:43.496911Z",
     "shell.execute_reply": "2022-05-20T15:12:43.495791Z",
     "shell.execute_reply.started": "2022-05-20T15:12:38.153629Z"
    },
    "papermill": {
     "duration": 5.416492,
     "end_time": "2022-04-12T14:48:28.96151",
     "exception": false,
     "start_time": "2022-04-12T14:48:23.545018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014416,
     "end_time": "2022-04-12T14:48:28.990998",
     "exception": false,
     "start_time": "2022-04-12T14:48:28.976582",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.2 PASCAL VOC 2009\n",
    "For this project you will be using the [PASCAL VOC 2009](http://host.robots.ox.ac.uk/pascal/VOC/voc2009/index.html) dataset. This dataset consists of colour images of various scenes with different object classes (e.g. animal: *bird, cat, ...*; vehicle: *aeroplane, bicycle, ...*), totalling 20 classes.\n",
    "\n",
    "Our dataset is relatively small: \n",
    "The training set contains 749 examples and the test set contains 750 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "1ce67f49-6bf6-4e5c-b5e4-576e893616a9",
    "_uuid": "3b1c5fbb-757f-4349-b224-e281c540e1ad",
    "execution": {
     "iopub.execute_input": "2022-05-20T15:12:43.499862Z",
     "iopub.status.busy": "2022-05-20T15:12:43.499519Z",
     "iopub.status.idle": "2022-05-20T15:13:06.385170Z",
     "shell.execute_reply": "2022-05-20T15:13:06.384129Z",
     "shell.execute_reply.started": "2022-05-20T15:12:43.499820Z"
    },
    "papermill": {
     "duration": 21.336481,
     "end_time": "2022-04-12T14:48:50.342062",
     "exception": false,
     "start_time": "2022-04-12T14:48:29.005581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Elina\n",
    "#initstring = ../input/InputData/\n",
    "\n",
    "# Nastya\n",
    "#initstring = './kaggle/input/'\n",
    "\n",
    "#Pili\n",
    "initstring = '/kaggle/input/'\n",
    "\n",
    "# Loading the training data\n",
    "train_df = pd.read_csv(f'{initstring}kul-h02a5a-computer-vision-ga2-2022/train/train_set.csv', index_col=\"Id\")\n",
    "labels = train_df.columns\n",
    "train_df[\"img\"] = [np.load('{}kul-h02a5a-computer-vision-ga2-2022/train/img/train_{}.npy'.format(initstring, idx)) for idx, _ in train_df.iterrows()]\n",
    "train_df[\"seg\"] = [np.load('{}kul-h02a5a-computer-vision-ga2-2022/train/seg/train_{}.npy'.format(initstring, idx)) for idx, _ in train_df.iterrows()]\n",
    "print(\"The training set contains {} examples.\".format(len(train_df)))\n",
    "\n",
    "# Show some examples\n",
    "fig, axs = plt.subplots(2, 20, figsize=(10 * 20, 10 * 2))\n",
    "for i, label in enumerate(labels):\n",
    "    df = train_df.loc[train_df[label] == 1]\n",
    "    axs[0, i].imshow(df.iloc[0][\"img\"], vmin=0, vmax=255)\n",
    "    axs[0, i].set_title(\"\\n\".join(label for label in labels if df.iloc[0][label] == 1), fontsize=40)\n",
    "    axs[0, i].axis(\"off\")\n",
    "    axs[1, i].imshow(df.iloc[0][\"seg\"], vmin=0, vmax=20)  # with the absolute color scale it will be clear that the arrays in the \"seg\" column are label maps (labels in [0, 20])\n",
    "    axs[1, i].axis(\"off\")\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "# The training dataframe contains for each image 20 columns with the ground truth classification labels and 20 column with the ground truth segmentation maps for each class\n",
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:06.386968Z",
     "iopub.status.busy": "2022-05-20T15:13:06.386621Z",
     "iopub.status.idle": "2022-05-20T15:13:18.665013Z",
     "shell.execute_reply": "2022-05-20T15:13:18.664046Z",
     "shell.execute_reply.started": "2022-05-20T15:13:06.386933Z"
    },
    "papermill": {
     "duration": 11.507733,
     "end_time": "2022-04-12T14:49:02.044233",
     "exception": false,
     "start_time": "2022-04-12T14:48:50.5365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading the test data\n",
    "test_df = pd.read_csv(f'{initstring}kul-h02a5a-computer-vision-ga2-2022/test/test_set.csv', index_col=\"Id\")\n",
    "test_df[\"img\"] = [np.load('{}kul-h02a5a-computer-vision-ga2-2022/test/img/test_{}.npy'.format(initstring, idx)) for idx, _ in test_df.iterrows()]\n",
    "test_df[\"seg\"] = [-1 * np.ones(img.shape[:2], dtype=np.int8) for img in test_df[\"img\"]]\n",
    "print(\"The test set contains {} examples.\".format(len(test_df)))\n",
    "\n",
    "# The test dataframe is similar to the training dataframe, but here the values are -1 --> your task is to fill in these as good as possible in Sect. 2 and Sect. 3; in Sect. 6 this dataframe is automatically transformed in the submission CSV!\n",
    "test_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.197841,
     "end_time": "2022-04-12T14:49:02.437252",
     "exception": false,
     "start_time": "2022-04-12T14:49:02.239411",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.3 Your Kaggle submission\n",
    "Your filled test dataframe (during Sect. 2 and Sect. 3) must be converted to a submission.csv with two rows per example (one for classification and one for segmentation) and with only a single prediction column (the multi-class/label predictions running length encoded). You don't need to edit this section. Just make sure to call this function at the right position in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:18.668399Z",
     "iopub.status.busy": "2022-05-20T15:13:18.668029Z",
     "iopub.status.idle": "2022-05-20T15:13:18.680988Z",
     "shell.execute_reply": "2022-05-20T15:13:18.679792Z",
     "shell.execute_reply.started": "2022-05-20T15:13:18.668354Z"
    },
    "papermill": {
     "duration": 0.213344,
     "end_time": "2022-04-12T14:49:02.848597",
     "exception": false,
     "start_time": "2022-04-12T14:49:02.635253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _rle_encode(img):\n",
    "    \"\"\"\n",
    "    Kaggle requires RLE encoded predictions for computation of the Dice score (https://www.kaggle.com/lifa08/run-length-encode-and-decode)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img: np.ndarray - binary img array\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    rle: String - running length encoded version of img\n",
    "    \"\"\"\n",
    "    pixels = img.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    rle = ' '.join(str(x) for x in runs)\n",
    "    return rle\n",
    "\n",
    "def generate_submission(df):\n",
    "    \"\"\"\n",
    "    Make sure to call this function once after you completed Sect. 2 and Sect. 3! It transforms and writes your test dataframe into a submission.csv file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame - filled dataframe that needs to be converted\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    submission_df: pd.DataFrame - df in submission format.\n",
    "    \"\"\"\n",
    "    df_dict = {\"Id\": [], \"Predicted\": []}\n",
    "    for idx, _ in df.iterrows():\n",
    "        df_dict[\"Id\"].append(f\"{idx}_classification\")\n",
    "        df_dict[\"Predicted\"].append(_rle_encode(np.array(df.loc[idx, labels])))\n",
    "        df_dict[\"Id\"].append(f\"{idx}_segmentation\")\n",
    "        df_dict[\"Predicted\"].append(_rle_encode(np.array([df.loc[idx, \"seg\"] == j + 1 for j in range(len(labels))])))\n",
    "    \n",
    "    submission_df = pd.DataFrame(data=df_dict, dtype=str).set_index(\"Id\")\n",
    "    submission_df.to_csv(\"submission.csv\")\n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balance\n",
    "\n",
    "We took a look at the number of object in each class and noticed that the category person has significantly more objects than the other class. This might give problems in the future classification since introduces a bias caused by class imbalance. Since our dataset is relatively small (training set contains 749 examples and the test set 750 examples), we decided to augment all the classes except person. This was done by flipping both the segmentation and original images and changing the brightness of the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:18.683583Z",
     "iopub.status.busy": "2022-05-20T15:13:18.683142Z",
     "iopub.status.idle": "2022-05-20T15:13:18.699121Z",
     "shell.execute_reply": "2022-05-20T15:13:18.697959Z",
     "shell.execute_reply.started": "2022-05-20T15:13:18.683522Z"
    }
   },
   "outputs": [],
   "source": [
    "def augmentData(data):\n",
    "    labels = data.columns\n",
    "    pp = labels.get_loc(\"person\") ##Position person\n",
    "    for i in range(data[\"img\"].shape[0]):\n",
    "        select= data.loc[i]\n",
    "        if select[labels[pp]]==0:\n",
    "        \n",
    "            FLR = select.copy()\n",
    "            FLR[\"img\"] = np.array(tf.image.flip_left_right(FLR[\"img\"]))\n",
    "            FLR[\"seg\"] = np.array(tf.image.flip_left_right(tf.expand_dims(FLR[\"seg\"],axis=2)))\n",
    "            data = data.append(FLR,ignore_index=True)\n",
    "        \n",
    "#             FUD = select.copy()\n",
    "#             FUD[\"img\"] = np.array(tf.image.flip_up_down(FUD[\"img\"]))\n",
    "#             FUD[\"seg\"] = np.array(tf.image.flip_up_down(tf.expand_dims(FUD[\"seg\"],axis=2)))\n",
    "#             data = data.append(FUD,ignore_index=True)\n",
    "        \n",
    "            B = select.copy()\n",
    "            B[\"img\"] = np.array(tf.image.random_brightness(B[\"img\"],0.6, seed=None))\n",
    "            data = data.append(B,ignore_index=True)\n",
    "            \n",
    "            B = select.copy()\n",
    "            B[\"img\"] = np.array(tf.image.random_brightness(B[\"img\"],0.4, seed=None))\n",
    "            data = data.append(B,ignore_index=True)\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resize\n",
    "The images did not have the same size at the beginning, so we searched for the biggest proportions (500x500) and resized the images to that particular size. Later on, we had memory problems with big RGB images, so we were forced to make these images smaller and resize them to 128x128.\n",
    "\n",
    "#### Normalise & Filtering\n",
    "A normalisation was also performed on the resized images, by dividing the array values by 255 as well as a gaussian blur filter. When these preprocessing steps were completed, we converted both the image en segmentation image to tensors, to make it possible to feed the system with it.\n",
    "\n",
    "#### Grayscale\n",
    "A discussion appeared when we wanted to feed the images to the neural network. Depending on the object of classification, grayscale or RGB should give better results. Since grayscale images are less costly than RGB, we first tried to augment grayscale images and use them to train on. We quickly understood that this results were less good than if we used augmented RGB images. This by the reason that our problem tries to classify objects, and their colour plays an important role as well in learning a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:18.703253Z",
     "iopub.status.busy": "2022-05-20T15:13:18.702712Z",
     "iopub.status.idle": "2022-05-20T15:13:18.722400Z",
     "shell.execute_reply": "2022-05-20T15:13:18.721342Z",
     "shell.execute_reply.started": "2022-05-20T15:13:18.703207Z"
    }
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    \n",
    "    def __init__(self, img_size, test):\n",
    "        self.img_size = img_size\n",
    "        self.width = self.img_size[1]\n",
    "        self.height = self.img_size[0]\n",
    "        self.test = test\n",
    "        \n",
    "    def resize(self,img):\n",
    "        resized = cv2.resize(img, (self.width,self.height), interpolation = cv2.INTER_AREA)\n",
    "        return resized\n",
    "    \n",
    "    def normalization(self,gray_img):\n",
    "        img = gray_img / 255.0\n",
    "        return img\n",
    "    \n",
    "    def Filter(self,img,filter):\n",
    "        if filter=='Gaussian':\n",
    "            img = cv2.GaussianBlur(img,(5,5),cv2.BORDER_DEFAULT)\n",
    "        elif filter=='Median':\n",
    "            img = cv2.medianBlur(img,5)\n",
    "        else:\n",
    "            raise Error\n",
    "            \n",
    "        return img\n",
    "    \n",
    "    def preprocess(self,data):\n",
    "        preprocData = data.copy()\n",
    "        images = data[\"img\"]\n",
    "        segmentations = data[\"seg\"]\n",
    "        nb_images = images.shape[0]\n",
    "        for i in range(nb_images):\n",
    "            image = images[i]\n",
    "            seg = segmentations[i]\n",
    "            # Resize both img and seg\n",
    "            image = self.resize(image)\n",
    "            if self.test == False:\n",
    "                seg = self.resize(seg)\n",
    "            # Normalization\n",
    "            normalized_img = self.normalization(image)\n",
    "            # Filter \n",
    "            blur_img = self.Filter(normalized_img,'Gaussian')\n",
    "            img = tf.convert_to_tensor(blur_img)\n",
    "            seg = tf.convert_to_tensor(seg)\n",
    "            images[i]=img\n",
    "            segmentations[i]=seg\n",
    "            \n",
    "        preprocData['img'] = images\n",
    "        preprocData['seg'] = segmentations\n",
    "        return preprocData\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        return self.preprocess(data)\n",
    "        \n",
    "\n",
    "def FindLargestDim(data):\n",
    "    imgs = data['img']\n",
    "    x_max = 0\n",
    "    y_max = 0\n",
    "    for img in imgs:\n",
    "        x,y,_ = img.shape\n",
    "        if x>x_max:\n",
    "            x_max=x\n",
    "        if y>y_max:\n",
    "            y_max=y\n",
    "    return (x_max,y_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:18.725048Z",
     "iopub.status.busy": "2022-05-20T15:13:18.724051Z",
     "iopub.status.idle": "2022-05-20T15:13:43.522698Z",
     "shell.execute_reply": "2022-05-20T15:13:43.521721Z",
     "shell.execute_reply.started": "2022-05-20T15:13:18.725004Z"
    }
   },
   "outputs": [],
   "source": [
    "# Balance data set\n",
    "# Choose if you want to use images with 3 dimensions (in RGB) or the grayscale images\n",
    "# For transfer learning we need images in RGB values\n",
    "preprocessed_train_df = train_df.copy()\n",
    "test = False\n",
    "\n",
    "preprocessed_train_df = augmentData(preprocessed_train_df)\n",
    "    \n",
    "# dim = FindLargestDim(preprocessed_train_df)\n",
    "dim = (128,128)\n",
    "preprocessor = Preprocessor(dim, test)\n",
    "preprocessed_train_df = preprocessor(preprocessed_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:43.524830Z",
     "iopub.status.busy": "2022-05-20T15:13:43.524391Z",
     "iopub.status.idle": "2022-05-20T15:13:43.776707Z",
     "shell.execute_reply": "2022-05-20T15:13:43.775738Z",
     "shell.execute_reply.started": "2022-05-20T15:13:43.524784Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(train_df['img']))\n",
    "print(len(preprocessed_train_df['img']))\n",
    "print(preprocessed_train_df['img'][600].shape)\n",
    "plt.imshow(preprocessed_train_df['img'][600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:43.779312Z",
     "iopub.status.busy": "2022-05-20T15:13:43.778210Z",
     "iopub.status.idle": "2022-05-20T15:13:43.812135Z",
     "shell.execute_reply": "2022-05-20T15:13:43.811157Z",
     "shell.execute_reply.started": "2022-05-20T15:13:43.779264Z"
    }
   },
   "outputs": [],
   "source": [
    "# checking how balanced the dataset becomes after preprocessing\n",
    "for i in range(len(preprocessed_train_df.columns)-2):\n",
    "    arr = preprocessed_train_df[preprocessed_train_df.columns[i]].to_numpy()\n",
    "    amount = np.count_nonzero(arr == 1)\n",
    "\n",
    "    print('Number of', preprocessed_train_df.columns[i], ':', amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the images need to be put in datastructures which are compatible with the CNN's used\n",
    "\n",
    "output is a so called one-hot encoding. One-hot encoding can be used when as loss the categorical-entropy is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:43.817596Z",
     "iopub.status.busy": "2022-05-20T15:13:43.816717Z",
     "iopub.status.idle": "2022-05-20T15:13:45.235787Z",
     "shell.execute_reply": "2022-05-20T15:13:45.234766Z",
     "shell.execute_reply.started": "2022-05-20T15:13:43.817551Z"
    }
   },
   "outputs": [],
   "source": [
    "SIZE_IMG = dim[0]\n",
    "\n",
    "y = preprocessed_train_df[['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']]\n",
    "train_yC = y.to_numpy()\n",
    "print(train_yC)\n",
    "\n",
    "# reshape images \n",
    "# input images should have a shape [batch_size, img_height, img_width, number_of_channels]\n",
    "numberOfIm = len(preprocessed_train_df[\"img\"])\n",
    "dimIm = 3\n",
    "    \n",
    "train_x = np.array(np.zeros([numberOfIm, SIZE_IMG, SIZE_IMG, dimIm]))\n",
    "for i in range(numberOfIm):\n",
    "    Im = preprocessed_train_df[\"img\"][i]\n",
    "    A = np.asarray(Im).reshape(SIZE_IMG,SIZE_IMG,dimIm)\n",
    "    train_x[i,:,:,:] = A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same preprocess procedure for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:45.237869Z",
     "iopub.status.busy": "2022-05-20T15:13:45.237551Z",
     "iopub.status.idle": "2022-05-20T15:13:47.683410Z",
     "shell.execute_reply": "2022-05-20T15:13:47.682264Z",
     "shell.execute_reply.started": "2022-05-20T15:13:45.237825Z"
    }
   },
   "outputs": [],
   "source": [
    "# same preprocessing procedure for the test data\n",
    "test = True\n",
    "\n",
    "preprocessed_test_df = test_df.copy()\n",
    "# dim = FindLargestDim(test_df)\n",
    "dim = (128,128)\n",
    "preprocessor = Preprocessor(dim, test)\n",
    "preprocessed_test_df = preprocessor(preprocessed_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:47.685438Z",
     "iopub.status.busy": "2022-05-20T15:13:47.685104Z",
     "iopub.status.idle": "2022-05-20T15:13:47.940252Z",
     "shell.execute_reply": "2022-05-20T15:13:47.939138Z",
     "shell.execute_reply.started": "2022-05-20T15:13:47.685395Z"
    }
   },
   "outputs": [],
   "source": [
    "print(preprocessed_test_df['img'][2].shape)\n",
    "print(len(preprocessed_test_df['img']))\n",
    "plt.imshow(preprocessed_test_df['img'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:47.942952Z",
     "iopub.status.busy": "2022-05-20T15:13:47.942226Z",
     "iopub.status.idle": "2022-05-20T15:13:48.577891Z",
     "shell.execute_reply": "2022-05-20T15:13:48.576717Z",
     "shell.execute_reply.started": "2022-05-20T15:13:47.942904Z"
    }
   },
   "outputs": [],
   "source": [
    "# reshape images \n",
    "# input images should have a shape [batch_size, img_height, img_width, number_of_channels]\n",
    "numberOfIm = len(preprocessed_test_df[\"img\"])\n",
    "test = np.array(np.zeros([numberOfIm, SIZE_IMG, SIZE_IMG, dimIm]))\n",
    "for i in range(numberOfIm):\n",
    "    Im = preprocessed_test_df[\"img\"][i]\n",
    "    A = np.asarray(Im).reshape(SIZE_IMG,SIZE_IMG,dimIm)\n",
    "    test[i,:,:,:] = A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:48.579687Z",
     "iopub.status.busy": "2022-05-20T15:13:48.579385Z",
     "iopub.status.idle": "2022-05-20T15:13:48.588090Z",
     "shell.execute_reply": "2022-05-20T15:13:48.587098Z",
     "shell.execute_reply.started": "2022-05-20T15:13:48.579643Z"
    }
   },
   "outputs": [],
   "source": [
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.196641,
     "end_time": "2022-04-12T14:49:03.240824",
     "exception": false,
     "start_time": "2022-04-12T14:49:03.044183",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Image classification\n",
    "The goal here is simple: implement a classification CNN and train it to recognise all 20 classes (and/or background) using the training set and compete on the test set (by filling in the classification columns in the test dataframe). We designed two main models, one model was build from scratch, and the other one uses transfer learning to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Image classification from scratch\n",
    "\n",
    "We tried several models and finally converged to the blog proposed by a following blog: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html . This model was finetuned and the hyperparameters were finetuned as much as possible. But still, after a lot of effort, the model could not find a relatively good solution. So we have decided to put more effort in the transfer learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Journey\n",
    "\n",
    "After several experiments, we converged that th adam optimizer was the best solution for our problem. The trick was to set the learning rate small enough to let it converge without using too much epochs.\n",
    "It is important to notice that the input are the resized images in RGB, and the output layer contains 20 classes. Most sources recommended to use a sigmoid function at the end and a binary cross-entropy loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1\n",
    "\n",
    "The goal here is simple: implement a classification CNN and train it to recognise all 20 classes (and/or background) using the training set and compete on the test set (by filling in the classification columns in the test dataframe).\n",
    "\n",
    "https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:51:28.736565Z",
     "iopub.status.busy": "2022-05-20T15:51:28.736034Z",
     "iopub.status.idle": "2022-05-20T15:51:28.746420Z",
     "shell.execute_reply": "2022-05-20T15:51:28.745237Z",
     "shell.execute_reply.started": "2022-05-20T15:51:28.736515Z"
    }
   },
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import matplotlib.image as mpimg\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:49.242958Z",
     "iopub.status.busy": "2022-05-20T15:13:49.242623Z",
     "iopub.status.idle": "2022-05-20T15:13:49.249254Z",
     "shell.execute_reply": "2022-05-20T15:13:49.247768Z",
     "shell.execute_reply.started": "2022-05-20T15:13:49.242915Z"
    }
   },
   "outputs": [],
   "source": [
    "# # model that was proposed in the blog\n",
    "# model = tf.keras.models.Sequential([\n",
    "#     tf.keras.layers.Conv2D(32,(3,3),activation = \"relu\" , input_shape = (128,128,3)) ,\n",
    "#     tf.keras.layers.MaxPooling2D(2,2),\n",
    "#     tf.keras.layers.Conv2D(32,(3,3),activation = \"relu\") ,  \n",
    "#     tf.keras.layers.MaxPooling2D(2,2),\n",
    "#     tf.keras.layers.Conv2D(64,(3,3),activation = \"relu\") ,  \n",
    "#     tf.keras.layers.MaxPooling2D(2,2),\n",
    "#     # added\n",
    "#     tf.keras.layers.Conv2D(64,(3,3),activation = \"relu\") ,  \n",
    "#     tf.keras.layers.AveragePooling2D(2,2),\n",
    "    \n",
    "#     tf.keras.layers.Flatten(), \n",
    "#     tf.keras.layers.Dense(64,activation=\"relu\"),      #Adding the Hidden layer\n",
    "#     tf.keras.layers.Dropout(0.2, seed = 2019),\n",
    "#     tf.keras.layers.Dense(20,activation = \"sigmoid\")   #Adding the Output Layer\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:49.251225Z",
     "iopub.status.busy": "2022-05-20T15:13:49.250733Z",
     "iopub.status.idle": "2022-05-20T15:13:49.262464Z",
     "shell.execute_reply": "2022-05-20T15:13:49.261328Z",
     "shell.execute_reply.started": "2022-05-20T15:13:49.251181Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Model 2\n",
    "# ## This is the first model we tried, the predictions are not that good. After 1 epoch overfit\n",
    "# model = tf.keras.models.Sequential([\n",
    "#     tf.keras.layers.Conv2D(16,(3,3),activation = \"relu\" , input_shape = (128,128,3)) ,\n",
    "#     tf.keras.layers.MaxPooling2D(2,2),\n",
    "#     tf.keras.layers.Conv2D(32,(3,3),activation = \"relu\") ,  \n",
    "#     tf.keras.layers.MaxPooling2D(2,2),\n",
    "#     tf.keras.layers.Conv2D(64,(3,3),activation = \"relu\") ,  \n",
    "#     tf.keras.layers.MaxPooling2D(2,2),\n",
    "#     tf.keras.layers.Conv2D(128,(3,3),activation = \"relu\"),  \n",
    "#     tf.keras.layers.MaxPooling2D(2,2),\n",
    "#     tf.keras.layers.Flatten(), \n",
    "#     tf.keras.layers.Dense(550,activation=\"relu\"),      #Adding the Hidden layer\n",
    "#     tf.keras.layers.Dropout(0.1,seed = 2019),\n",
    "#     #tf.keras.layers.Dense(400,activation =\"relu\"),\n",
    "#     #tf.keras.layers.Dropout(0.3,seed = 2019),\n",
    "#     #tf.keras.layers.Dense(300,activation=\"relu\"),\n",
    "#     #tf.keras.layers.Dropout(0.4,seed = 2019),\n",
    "#     #tf.keras.layers.Dense(200,activation =\"relu\"),\n",
    "#     #tf.keras.layers.Dropout(0.2,seed = 2019),\n",
    "#     tf.keras.layers.Dense(20,activation = \"sigmoid\")   #Adding the Output Layer: kleine getalletjes want absolute kansen (softmaxgeeft relatieve kansen, dus is niet dat hij heel onaccuraat is)\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:49.264721Z",
     "iopub.status.busy": "2022-05-20T15:13:49.264242Z",
     "iopub.status.idle": "2022-05-20T15:13:49.276369Z",
     "shell.execute_reply": "2022-05-20T15:13:49.275390Z",
     "shell.execute_reply.started": "2022-05-20T15:13:49.264675Z"
    }
   },
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[COPYRIGHT] So far, we've seen RMSProp and Momentum take contrasting approaches. While momentum accelerates our search in direction of minima, RMSProp impedes our search in direction of oscillations. Adam or Adaptive Moment Optimization algorithms combines the heuristics of both Momentum and RMSProp.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:49.278522Z",
     "iopub.status.busy": "2022-05-20T15:13:49.278098Z",
     "iopub.status.idle": "2022-05-20T15:13:49.287202Z",
     "shell.execute_reply": "2022-05-20T15:13:49.286187Z",
     "shell.execute_reply.started": "2022-05-20T15:13:49.278471Z"
    }
   },
   "outputs": [],
   "source": [
    "# # binary cross entropy: multiple labels per photo 1 000.  11000\n",
    "# from tensorflow.keras.optimizers import RMSprop,SGD,Adam\n",
    "\n",
    "# # TODO: overfit wrsch: binary of cross: want hoe bepaal je accuracty en loss nu? want multilabel nu, \n",
    "# # maar kijkt hij naar 1 of 2 values nu?\n",
    "# # probleem: val los wordt hoger ipv lager$\n",
    "\n",
    "# # adam: lr groot -> verklein. 0.1 -> 0.001 -> 0.000001\n",
    "# # wat is rmsprop -> opzoeken (adam goed)\n",
    "# adam = Adam(lr=0.01)\n",
    "\n",
    "# model.compile(#optimizer='rmsprop',\n",
    "#               optimizer='adam', \n",
    "#               loss='binary_crossentropy', \n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:49.289359Z",
     "iopub.status.busy": "2022-05-20T15:13:49.288994Z",
     "iopub.status.idle": "2022-05-20T15:13:49.298214Z",
     "shell.execute_reply": "2022-05-20T15:13:49.297101Z",
     "shell.execute_reply.started": "2022-05-20T15:13:49.289315Z"
    }
   },
   "outputs": [],
   "source": [
    "# # This callback will stop the training when there is no improvement in\n",
    "# # the loss for three consecutive epochs.\n",
    "# callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# checkpoint_filepath = 'checkpoint.h5'\n",
    "# # Model weights are saved at the end of every epoch, if it's the best seen so far.\n",
    "# model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#     filepath=checkpoint_filepath,\n",
    "#     save_weights_only=True,\n",
    "#     monitor='val_accuracy',\n",
    "#     mode='max',\n",
    "#     save_best_only=True)\n",
    "\n",
    "\n",
    "# history = model.fit(train_x, train_yC, validation_split=0.2, epochs=500, batch_size=32, verbose=1, callbacks=[callback, model_checkpoint_callback])\n",
    "\n",
    "# # The model weights (that are considered the best) are loaded into the model.\n",
    "# model.load_weights(checkpoint_filepath)\n",
    "\n",
    "# model.save('model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:49.300502Z",
     "iopub.status.busy": "2022-05-20T15:13:49.300143Z",
     "iopub.status.idle": "2022-05-20T15:13:49.313421Z",
     "shell.execute_reply": "2022-05-20T15:13:49.312248Z",
     "shell.execute_reply.started": "2022-05-20T15:13:49.300457Z"
    }
   },
   "outputs": [],
   "source": [
    "# # plotting the accuracy and loss functions\n",
    "# # list all data in history\n",
    "# print(history.history.keys())\n",
    "# # summarize history for accuracy\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])   # Terug toeveogen wanneer validatieset hebben\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "# # summarize history for loss\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:49.315981Z",
     "iopub.status.busy": "2022-05-20T15:13:49.314950Z",
     "iopub.status.idle": "2022-05-20T15:13:49.324147Z",
     "shell.execute_reply": "2022-05-20T15:13:49.323021Z",
     "shell.execute_reply.started": "2022-05-20T15:13:49.315931Z"
    }
   },
   "outputs": [],
   "source": [
    "#import keras\n",
    "#model = keras.models.load_model('ModelFromScracth.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:49.328073Z",
     "iopub.status.busy": "2022-05-20T15:13:49.325481Z",
     "iopub.status.idle": "2022-05-20T15:13:49.334380Z",
     "shell.execute_reply": "2022-05-20T15:13:49.333299Z",
     "shell.execute_reply.started": "2022-05-20T15:13:49.328040Z"
    }
   },
   "outputs": [],
   "source": [
    "# # make a prediction on the test set\n",
    "# yhat = model.predict(test)\n",
    "# print(yhat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:49.336815Z",
     "iopub.status.busy": "2022-05-20T15:13:49.336058Z",
     "iopub.status.idle": "2022-05-20T15:13:49.346973Z",
     "shell.execute_reply": "2022-05-20T15:13:49.345938Z",
     "shell.execute_reply.started": "2022-05-20T15:13:49.336770Z"
    }
   },
   "outputs": [],
   "source": [
    "# # choose threshold to convert to zero's and ones\n",
    "# yhat = yhat > 0.1\n",
    "# yhat = yhat.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:49.349345Z",
     "iopub.status.busy": "2022-05-20T15:13:49.348903Z",
     "iopub.status.idle": "2022-05-20T15:13:49.357201Z",
     "shell.execute_reply": "2022-05-20T15:13:49.356160Z",
     "shell.execute_reply.started": "2022-05-20T15:13:49.349300Z"
    }
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import numpy\n",
    "# numpy.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "# # yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:49.359725Z",
     "iopub.status.busy": "2022-05-20T15:13:49.359306Z",
     "iopub.status.idle": "2022-05-20T15:13:49.367285Z",
     "shell.execute_reply": "2022-05-20T15:13:49.366414Z",
     "shell.execute_reply.started": "2022-05-20T15:13:49.359590Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## Testing with 1 image\n",
    "# plt.imshow(test_df[\"img\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:49.369812Z",
     "iopub.status.busy": "2022-05-20T15:13:49.369387Z",
     "iopub.status.idle": "2022-05-20T15:13:49.377349Z",
     "shell.execute_reply": "2022-05-20T15:13:49.376420Z",
     "shell.execute_reply.started": "2022-05-20T15:13:49.369766Z"
    }
   },
   "outputs": [],
   "source": [
    "# testImage = test[1,:,:,:]\n",
    "# pred = model.predict(testImage.reshape(1,128,128,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:49.386254Z",
     "iopub.status.busy": "2022-05-20T15:13:49.384822Z",
     "iopub.status.idle": "2022-05-20T15:13:49.390831Z",
     "shell.execute_reply": "2022-05-20T15:13:49.389521Z",
     "shell.execute_reply.started": "2022-05-20T15:13:49.386205Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(pred)\n",
    "# print(np.argmax(pred))\n",
    "# top = np.argsort(pred[0])[:-4:-1]\n",
    "# print(top)\n",
    "# for i in range(len(top)):\n",
    "#     index = top[i]\n",
    "#     print(train_df.columns[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:49.393017Z",
     "iopub.status.busy": "2022-05-20T15:13:49.392471Z",
     "iopub.status.idle": "2022-05-20T15:13:49.405450Z",
     "shell.execute_reply": "2022-05-20T15:13:49.404002Z",
     "shell.execute_reply.started": "2022-05-20T15:13:49.392972Z"
    }
   },
   "outputs": [],
   "source": [
    "# plt.imshow(test_df[\"img\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:49.407995Z",
     "iopub.status.busy": "2022-05-20T15:13:49.406968Z",
     "iopub.status.idle": "2022-05-20T15:13:49.417088Z",
     "shell.execute_reply": "2022-05-20T15:13:49.415892Z",
     "shell.execute_reply.started": "2022-05-20T15:13:49.407948Z"
    }
   },
   "outputs": [],
   "source": [
    "# testImage2 = test[2,:,:,:]\n",
    "# pred = model.predict(testImage2.reshape(1,128,128,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:49.419923Z",
     "iopub.status.busy": "2022-05-20T15:13:49.419111Z",
     "iopub.status.idle": "2022-05-20T15:13:49.428379Z",
     "shell.execute_reply": "2022-05-20T15:13:49.427291Z",
     "shell.execute_reply.started": "2022-05-20T15:13:49.419878Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(pred)\n",
    "# print(np.argmax(pred))\n",
    "# top = np.argsort(pred)[:-4:-1]\n",
    "# print(top)\n",
    "# for i in range(len(top)):\n",
    "#     index = top[i]\n",
    "#     print(train_df.columns[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:49.432969Z",
     "iopub.status.busy": "2022-05-20T15:13:49.432158Z",
     "iopub.status.idle": "2022-05-20T15:13:49.439715Z",
     "shell.execute_reply": "2022-05-20T15:13:49.438613Z",
     "shell.execute_reply.started": "2022-05-20T15:13:49.432932Z"
    }
   },
   "outputs": [],
   "source": [
    "# plt.imshow(test_df[\"img\"][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:49.442172Z",
     "iopub.status.busy": "2022-05-20T15:13:49.441621Z",
     "iopub.status.idle": "2022-05-20T15:13:49.451702Z",
     "shell.execute_reply": "2022-05-20T15:13:49.450623Z",
     "shell.execute_reply.started": "2022-05-20T15:13:49.442125Z"
    }
   },
   "outputs": [],
   "source": [
    "# testImage3 = test[3,:,:,:]\n",
    "# pred = model.predict(testImage2.reshape(1,128,128,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:49.454168Z",
     "iopub.status.busy": "2022-05-20T15:13:49.453821Z",
     "iopub.status.idle": "2022-05-20T15:13:49.462646Z",
     "shell.execute_reply": "2022-05-20T15:13:49.461543Z",
     "shell.execute_reply.started": "2022-05-20T15:13:49.454101Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(pred)\n",
    "# print(np.argmax(pred))\n",
    "# top = np.argsort(pred[0])[:-4:-1]\n",
    "# print(top)\n",
    "# for i in range(len(top)):\n",
    "#     index = top[i]\n",
    "#     print(train_df.columns[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Image classification with transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For transfer learning we tried a lot of different models and added a lot of different layers. The model which performed the best is given below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:49.465032Z",
     "iopub.status.busy": "2022-05-20T15:13:49.464402Z",
     "iopub.status.idle": "2022-05-20T15:13:49.858189Z",
     "shell.execute_reply": "2022-05-20T15:13:49.857010Z",
     "shell.execute_reply.started": "2022-05-20T15:13:49.464852Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications import ResNet101\n",
    "from tensorflow.keras.models import Model, load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As base model we tried ResNet50, ResNet101, VGG16, VGG19, InceptionV3 and DenseNet. Since DenseNet gave the best results, we chose this as the base model. To the base model we added following layers:\n",
    "* Flatten: This layer makes from a matrix a array. This ensure the input has the right shape.\n",
    "* Batchnormalization: This layer applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1. The purpose of this is to avoid exploding or fanishing gradients, which happens when the gradient becomes too large or too small.\n",
    "* Dense: A normal layer which calculates the activation function of the input multiplied with the weight of the corresponding neuron. \n",
    "* Dropout: The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting. Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged.\n",
    "* Dense: again a normal layer to calculate the activation function, however this time the activation function is set to 'sigmoid', a non-linear function. This is a good last activation function for the type of output used in this assignment.\n",
    "\n",
    "After constructing the model, the model needs to be compiled. The Adam function to minimize the loss gave the best results. \n",
    "\n",
    "After compiling the function the training images and corresponding labels are put togheter in one dataset. This approach proved the give better results than when we keep them apart. Here the trainig set is also split in a training and validation set. The validation set is needed to prevent overfitting. Overfitting happens when a model follows exactly the training samples and doesn't generalise well. Since the model is not trained on the validation set, the loss of the model on the validation set will get worse when the model is overfitted. Therefore we know the training is converged as soon as the loss of the validation set starts to increase instead of decrease. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:49.862056Z",
     "iopub.status.busy": "2022-05-20T15:13:49.861816Z",
     "iopub.status.idle": "2022-05-20T15:13:54.883643Z",
     "shell.execute_reply": "2022-05-20T15:13:54.882607Z",
     "shell.execute_reply.started": "2022-05-20T15:13:49.862025Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "K.set_learning_phase(0)\n",
    "\n",
    "numClasses = 20\n",
    "\n",
    "# base_model = ResNet101(weights=\"imagenet\", include_top=False, pooling='max', input_shape=(128, 128, 3))\n",
    "# for layer in base_model.layers:\n",
    "#     layer.trainable = False\n",
    "base_model=tf.keras.applications.DenseNet121(input_shape=[128,128,3], \n",
    "                                       include_top=False, \n",
    "                                       weights='imagenet') \n",
    "    \n",
    "K.set_learning_phase(1)\n",
    "\n",
    "top_model = Sequential()\n",
    "#vgg16 = tf.keras.applications.vgg16\n",
    "#transferModel.add(vgg16.VGG16(weights='imagenet', include_top=False, input_shape=(500, 500, 3))) -> kernel dies\n",
    "#transferModel.add(ResNet50(weights=\"imagenet\", include_top=False, pooling='max', input_shape=(500, 500, 3)))\n",
    "top_model.add(Flatten())\n",
    "top_model.add(BatchNormalization(renorm=True))\n",
    "top_model.add(Dense(512, activation='relu'))\n",
    "top_model.add(Dropout(0.1))\n",
    "# top_model.add(Dense(216, activation='relu'))\n",
    "top_model.add(Dense(numClasses, activation='sigmoid'))\n",
    "#transferModel.layers[0].trainable = False\n",
    "\n",
    "transferModel = Model(base_model.input, top_model(base_model.output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:54.885384Z",
     "iopub.status.busy": "2022-05-20T15:13:54.885055Z",
     "iopub.status.idle": "2022-05-20T15:13:54.889858Z",
     "shell.execute_reply": "2022-05-20T15:13:54.888677Z",
     "shell.execute_reply.started": "2022-05-20T15:13:54.885342Z"
    }
   },
   "outputs": [],
   "source": [
    "# transferModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:54.891985Z",
     "iopub.status.busy": "2022-05-20T15:13:54.891454Z",
     "iopub.status.idle": "2022-05-20T15:13:54.931210Z",
     "shell.execute_reply": "2022-05-20T15:13:54.930190Z",
     "shell.execute_reply.started": "2022-05-20T15:13:54.891940Z"
    }
   },
   "outputs": [],
   "source": [
    "adam=tf.keras.optimizers.Adam(lr=1e-6)\n",
    "transferModel.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:54.933719Z",
     "iopub.status.busy": "2022-05-20T15:13:54.933046Z",
     "iopub.status.idle": "2022-05-20T15:13:58.108439Z",
     "shell.execute_reply": "2022-05-20T15:13:58.107075Z",
     "shell.execute_reply.started": "2022-05-20T15:13:54.933673Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "trainsplit_XC, val_XC,trainsplit_yC, val_yC = train_test_split(train_x,train_yC, \n",
    "                                                   test_size=0.2, \n",
    "                                                   random_state=0\n",
    "                                                  )\n",
    "train_dataC = tf.data.Dataset.from_tensor_slices((trainsplit_XC, trainsplit_yC))\n",
    "test_dataC = tf.data.Dataset.from_tensor_slices((val_XC, val_yC))\n",
    "BATCH_SIZE = 12\n",
    "BUFFER_SIZE = 1000\n",
    "train_batchesC = train_dataC.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "train_batchesC = train_batchesC.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "test_batchesC = test_dataC.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "test_batchesC = test_batchesC.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:13:58.110664Z",
     "iopub.status.busy": "2022-05-20T15:13:58.110129Z",
     "iopub.status.idle": "2022-05-20T15:27:20.249670Z",
     "shell.execute_reply": "2022-05-20T15:27:20.248703Z",
     "shell.execute_reply.started": "2022-05-20T15:13:58.110460Z"
    }
   },
   "outputs": [],
   "source": [
    "history = transferModel.fit(train_batchesC,validation_data=test_batchesC,steps_per_epoch=trainsplit_XC.shape[0]//BATCH_SIZE, epochs=50,validation_steps=val_XC.shape[0]//BATCH_SIZE, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shows plots of the accuracy and the loss. As can be seen on the plots the validation loss starts increasing again after 40 epochs. This means the model will start to overfit and we should stop the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T07:32:42.045222Z",
     "iopub.status.busy": "2022-05-20T07:32:42.044934Z",
     "iopub.status.idle": "2022-05-20T07:32:42.415943Z",
     "shell.execute_reply": "2022-05-20T07:32:42.415261Z",
     "shell.execute_reply.started": "2022-05-20T07:32:42.045187Z"
    }
   },
   "outputs": [],
   "source": [
    "# plotting the accuracy and loss functions\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])   # Terug toeveogen wanneer validatieset hebben\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to look at some results. An image of the test set is shown below. There is a sofa on the image visible. The ouput of the prediction is the chance the image contains the object represented by the label. The three labels which had the biggest chance of being in the image contains also a sofa. Although this didn't get the highest chance, the model still got close. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T07:56:58.035233Z",
     "iopub.status.busy": "2022-05-20T07:56:58.034972Z",
     "iopub.status.idle": "2022-05-20T07:56:58.268112Z",
     "shell.execute_reply": "2022-05-20T07:56:58.267420Z",
     "shell.execute_reply.started": "2022-05-20T07:56:58.035205Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(test_df[\"img\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T07:57:01.343787Z",
     "iopub.status.busy": "2022-05-20T07:57:01.343388Z",
     "iopub.status.idle": "2022-05-20T07:57:01.429091Z",
     "shell.execute_reply": "2022-05-20T07:57:01.428345Z",
     "shell.execute_reply.started": "2022-05-20T07:57:01.343738Z"
    }
   },
   "outputs": [],
   "source": [
    "testImage = test[2,:,:,:]\n",
    "pred = transferModel.predict(testImage.reshape(1,128,128,3))\n",
    "top = np.argsort(pred[0])[:-4:-1]\n",
    "for i in range(len(top)):\n",
    "    index = top[i]\n",
    "    print(train_df.columns[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try another image. Image 63 contains a cat as can be seen below. The model did a really good prediction for this one. As the highest chance of the image containing a label was given to the label of the cat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T08:12:00.461351Z",
     "iopub.status.busy": "2022-05-20T08:12:00.460521Z",
     "iopub.status.idle": "2022-05-20T08:12:00.683911Z",
     "shell.execute_reply": "2022-05-20T08:12:00.683012Z",
     "shell.execute_reply.started": "2022-05-20T08:12:00.461313Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(test_df[\"img\"][63])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T08:12:18.529280Z",
     "iopub.status.busy": "2022-05-20T08:12:18.528661Z",
     "iopub.status.idle": "2022-05-20T08:12:18.603454Z",
     "shell.execute_reply": "2022-05-20T08:12:18.602635Z",
     "shell.execute_reply.started": "2022-05-20T08:12:18.529242Z"
    }
   },
   "outputs": [],
   "source": [
    "testImage = test[63,:,:,:]\n",
    "pred = transferModel.predict(testImage.reshape(1,128,128,3))\n",
    "top = np.argsort(pred[0])[:-4:-1]\n",
    "for i in range(len(top)):\n",
    "    index = top[i]\n",
    "    print(train_df.columns[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A last example is a image with a dog on the beach. The model did not perform well for this image, since it predicts the image contains a aeroplane, bird and a boat. The image could be difficult to classify because the dog is not that different from the water. The colors look a bit the same and the water is not smooth behind the dog. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T08:15:05.854055Z",
     "iopub.status.busy": "2022-05-20T08:15:05.853443Z",
     "iopub.status.idle": "2022-05-20T08:15:06.084327Z",
     "shell.execute_reply": "2022-05-20T08:15:06.083653Z",
     "shell.execute_reply.started": "2022-05-20T08:15:05.854016Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(test_df[\"img\"][28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T08:15:10.579245Z",
     "iopub.status.busy": "2022-05-20T08:15:10.577524Z",
     "iopub.status.idle": "2022-05-20T08:15:10.653184Z",
     "shell.execute_reply": "2022-05-20T08:15:10.652275Z",
     "shell.execute_reply.started": "2022-05-20T08:15:10.579187Z"
    }
   },
   "outputs": [],
   "source": [
    "testImage = test[28,:,:,:]\n",
    "pred = transferModel.predict(testImage.reshape(1,128,128,3))\n",
    "top = np.argsort(pred[0])[:-4:-1]\n",
    "for i in range(len(top)):\n",
    "    index = top[i]\n",
    "    print(train_df.columns[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the output of the model is processed so it can put in the csv file. The output of the model are all chances, so the ouput needs to be converted to zeros and ones. As some images contain multiple labels the three highest labels are set to one, the other labels are set to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T08:20:18.211537Z",
     "iopub.status.busy": "2022-05-20T08:20:18.211133Z",
     "iopub.status.idle": "2022-05-20T08:20:19.958629Z",
     "shell.execute_reply": "2022-05-20T08:20:19.957865Z",
     "shell.execute_reply.started": "2022-05-20T08:20:18.211501Z"
    }
   },
   "outputs": [],
   "source": [
    "yhat = transferModel.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-19T12:45:34.916975Z",
     "iopub.status.busy": "2022-05-19T12:45:34.916377Z",
     "iopub.status.idle": "2022-05-19T12:45:34.924564Z",
     "shell.execute_reply": "2022-05-19T12:45:34.923542Z",
     "shell.execute_reply.started": "2022-05-19T12:45:34.916924Z"
    }
   },
   "outputs": [],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-19T10:20:38.234468Z",
     "iopub.status.busy": "2022-05-19T10:20:38.2342Z",
     "iopub.status.idle": "2022-05-19T10:20:38.240292Z",
     "shell.execute_reply": "2022-05-19T10:20:38.23911Z",
     "shell.execute_reply.started": "2022-05-19T10:20:38.234436Z"
    }
   },
   "outputs": [],
   "source": [
    "# yhat = yhat > 0.99999\n",
    "# yhat = yhat.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-19T12:49:06.763812Z",
     "iopub.status.busy": "2022-05-19T12:49:06.763122Z",
     "iopub.status.idle": "2022-05-19T12:49:06.771516Z",
     "shell.execute_reply": "2022-05-19T12:49:06.770781Z",
     "shell.execute_reply.started": "2022-05-19T12:49:06.763771Z"
    }
   },
   "outputs": [],
   "source": [
    "print(labels)\n",
    "yhat[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-19T12:49:00.339030Z",
     "iopub.status.busy": "2022-05-19T12:49:00.338434Z",
     "iopub.status.idle": "2022-05-19T12:49:00.595309Z",
     "shell.execute_reply": "2022-05-19T12:49:00.591136Z",
     "shell.execute_reply.started": "2022-05-19T12:49:00.338988Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(test_df[\"img\"][7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-19T12:46:01.516741Z",
     "iopub.status.busy": "2022-05-19T12:46:01.516428Z",
     "iopub.status.idle": "2022-05-19T12:46:01.539489Z",
     "shell.execute_reply": "2022-05-19T12:46:01.538525Z",
     "shell.execute_reply.started": "2022-05-19T12:46:01.516705Z"
    }
   },
   "outputs": [],
   "source": [
    "for j in range(yhat.shape[0]):\n",
    "# for j in range(2):\n",
    "    top = np.argsort(yhat[j])[:-4:-1]\n",
    "    for i in range(len(top)):\n",
    "        index = top[i]\n",
    "        yhat[j,index]=1\n",
    "        \n",
    "yhat = yhat == 1\n",
    "yhat = yhat.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.19763,
     "end_time": "2022-04-12T14:49:07.53601",
     "exception": false,
     "start_time": "2022-04-12T14:49:07.33838",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Semantic segmentation\n",
    "The goal here is to implement a segmentation CNN that labels every pixel in the image as belonging to one of the 20 classes (and/or background). Use the training set to train your CNN and compete on the test set (by filling in the segmentation column in the test dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T18:47:57.13514Z",
     "iopub.status.busy": "2022-05-18T18:47:57.13491Z",
     "iopub.status.idle": "2022-05-18T18:48:06.753208Z",
     "shell.execute_reply": "2022-05-18T18:48:06.752492Z",
     "shell.execute_reply.started": "2022-05-18T18:47:57.135106Z"
    },
    "papermill": {
     "duration": 11.823715,
     "end_time": "2022-04-12T14:49:19.557577",
     "exception": false,
     "start_time": "2022-04-12T14:49:07.733862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RandomSegmentationModel:\n",
    "    \"\"\"\n",
    "    Random segmentation model: \n",
    "        - generates random label maps for the inputs based on the class distributions observed during training\n",
    "        - every pixel in an input can only have one label\n",
    "    \"\"\"\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Adjusts the class ratio variable to the one observed in Y. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: list of arrays - n x (height x width x 3)\n",
    "        Y: list of arrays - n x (height x width)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        self.distribution = np.mean([[np.sum(Y_ == i) / Y_.size for i in range(len(labels) + 1)] for Y_ in Y], axis=0)\n",
    "        print(\"Setting class distribution to:\\nbackground: {}\\n{}\".format(self.distribution[0], \"\\n\".join(f\"{label}: {p}\" for label, p in zip(labels, self.distribution[1:]))))\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts for each input a label map.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: list of arrays - n x (height x width x 3)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Y_pred: list of arrays - n x (height x width)\n",
    "        \"\"\"\n",
    "        np.random.seed(0)\n",
    "        return [np.random.choice(np.arange(len(labels) + 1), size=X_.shape[:2], p=self.distribution) for X_ in X]\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return self.predict(X)\n",
    "    \n",
    "model = RandomSegmentationModel()\n",
    "model.fit(train_df[\"img\"], train_df[\"seg\"])\n",
    "test_df.loc[:, \"seg\"] = model.predict(test_df[\"img\"])\n",
    "test_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Training-data for semantic segmentation\n",
    "For semantic segmentation the training-data for the model is a tensorflow dataset. The trainig data is split into a trainig set and a validation set. Further, the images and the segmentation images are set into a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-19T12:49:46.105752Z",
     "iopub.status.busy": "2022-05-19T12:49:46.104956Z",
     "iopub.status.idle": "2022-05-19T12:49:46.796875Z",
     "shell.execute_reply": "2022-05-19T12:49:46.796038Z",
     "shell.execute_reply.started": "2022-05-19T12:49:46.105709Z"
    }
   },
   "outputs": [],
   "source": [
    "dimImy = 1\n",
    "train_yS = np.array(np.zeros([preprocessed_train_df[\"seg\"].shape[0], dim[0], dim[1], dimImy]))\n",
    "for i in range(preprocessed_train_df[\"seg\"].shape[0]):\n",
    "    Im = preprocessed_train_df[\"seg\"][i]\n",
    "    A = np.asarray(Im).reshape(dim[0],dim[1],dimImy)\n",
    "    train_yS[i,:,:,:] = A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-19T12:49:50.316440Z",
     "iopub.status.busy": "2022-05-19T12:49:50.316116Z",
     "iopub.status.idle": "2022-05-19T12:49:51.376183Z",
     "shell.execute_reply": "2022-05-19T12:49:51.375237Z",
     "shell.execute_reply.started": "2022-05-19T12:49:50.316405Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "trainsplit_X, val_X,trainsplit_y, val_y = train_test_split(train_x,train_yS, \n",
    "                                                   test_size=0.2, \n",
    "                                                   random_state=0\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-19T12:49:52.592190Z",
     "iopub.status.busy": "2022-05-19T12:49:52.591924Z",
     "iopub.status.idle": "2022-05-19T12:49:54.820186Z",
     "shell.execute_reply": "2022-05-19T12:49:54.819413Z",
     "shell.execute_reply.started": "2022-05-19T12:49:52.592161Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((trainsplit_X, trainsplit_y))\n",
    "val_data = tf.data.Dataset.from_tensor_slices((val_X, val_y))\n",
    "BATCH_SIZE = 12\n",
    "BUFFER_SIZE = 1000\n",
    "train_batches = train_data.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "train_batches = train_batches.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "val_batches = val_data.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "val_batches = val_batches.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Model from scratch\n",
    "The model made from scratch is U-Net. This type of framework has showcased good ability for sematic segmentation. We used pixel based method and not contour based method.\n",
    "For the U-net model we resized all the images to 128x128x3 size and gave them as input to the model. Our model is made up of down-block, up-block and bottleneck.\n",
    "The down-block is used for : downsampling of the network. we used a kernel of 3x3 and a dropout of 0.05. For a higher value of dropout we had much worse results.\n",
    "The up-block is used for: we used this to have bigger layers in our CNN, so as to up size it. Again we keep then dropout layer to 0.05.\n",
    "\n",
    "In our model we have used 11 convoltion layers where we increase the size of neurons from 16, 32, 64, 128, 256 and bottelneck at 512. Then we go down again to 256 128 64 32 and 16.\n",
    "The final output is a tensor of 128x128x21 with 21 being the classes including the background. This output contains probabilities of every pixel belonging to that particular class.\n",
    "We take the class value for every pixel which has the highest probablility.\n",
    "\n",
    "The optimizer we used was Adam. The parameters for the models were choosen after perfroming several experiments. Our current model is able to detect the presence of an object but sometimes is not able to classify it correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-19T12:49:57.115227Z",
     "iopub.status.busy": "2022-05-19T12:49:57.114441Z",
     "iopub.status.idle": "2022-05-19T12:49:57.119929Z",
     "shell.execute_reply": "2022-05-19T12:49:57.118770Z",
     "shell.execute_reply.started": "2022-05-19T12:49:57.115188Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-19T12:49:59.329464Z",
     "iopub.status.busy": "2022-05-19T12:49:59.329176Z",
     "iopub.status.idle": "2022-05-19T12:49:59.357043Z",
     "shell.execute_reply": "2022-05-19T12:49:59.355821Z",
     "shell.execute_reply.started": "2022-05-19T12:49:59.329430Z"
    }
   },
   "outputs": [],
   "source": [
    "class Unet_model:\n",
    "    def __init__(self, input_shape):\n",
    "            self.input_shape=input_shape\n",
    "            \n",
    "    def down_block(self,x,size):\n",
    "        conv = tf.keras.layers.Conv2D(size, (3, 3), kernel_initializer='he_normal',\n",
    "                                    padding='same')(x)\n",
    "        conv = tf.keras.layers.Activation('relu')(conv)\n",
    "        conv = tf.keras.layers.Dropout(0.05)(conv)\n",
    "\n",
    "        conv = tf.keras.layers.Conv2D(size, (3, 3), activation=tf.keras.activations.relu, kernel_initializer='he_normal',\n",
    "                                    padding='same')(conv)\n",
    "        pool = tf.keras.layers.MaxPooling2D((2, 2))(conv)\n",
    "        return pool,conv\n",
    "\n",
    "    def bottleneck(self,x,size):\n",
    "        conv = tf.keras.layers.Conv2D(size, (3, 3), kernel_initializer='he_normal',\n",
    "                                    padding='same')(x)\n",
    "        conv = tf.keras.layers.Activation('relu')(conv)\n",
    "        conv = tf.keras.layers.Dropout(0.05)(conv)\n",
    "        conv = tf.keras.layers.Conv2D(size, (3, 3), activation=tf.keras.activations.relu, kernel_initializer='he_normal',\n",
    "                                    padding='same')(conv)\n",
    "        return conv\n",
    "\n",
    "    def up_block(self,x,y,size):\n",
    "    #     upconv = tf.keras.layers.Conv2DTranspose(size, (2, 2), strides=(2, 2), padding='same')(x)\n",
    "        upconv = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
    "        upconv = tf.keras.layers.concatenate([upconv, y])\n",
    "        conv = tf.keras.layers.Conv2D(size, (3, 3), kernel_initializer='he_normal',\n",
    "                                    padding='same')(upconv)\n",
    "        conv = tf.keras.layers.Activation('relu')(conv)\n",
    "        conv = tf.keras.layers.Dropout(0.05)(conv)\n",
    "        conv = tf.keras.layers.Conv2D(size, (3, 3), activation=tf.keras.activations.relu, kernel_initializer='he_normal',\n",
    "                                    padding='same')(conv)\n",
    "        return conv\n",
    "\n",
    "    def Model(self):\n",
    "        \"\"\"\n",
    "        Function to build the UNet model that was used for the own model in image segemntation\n",
    "        We experimented with both dropout and batchnormalization\n",
    "        In some experiments the dropout and or batchnormalization was removed or tweaked\n",
    "        \"\"\"\n",
    "        inputs = Input(self.input_shape)\n",
    "\n",
    "        pool1,conv1 = self.down_block(inputs,16)\n",
    "        pool2,conv2 = self.down_block(pool1,32)\n",
    "        pool3,conv3 = self.down_block(pool2,64)\n",
    "        pool4,conv4 = self.down_block(pool3,128)\n",
    "        pool5,conv5 = self.down_block(pool4,256)\n",
    "\n",
    "        conv6 = self.bottleneck(pool5,512)\n",
    "\n",
    "        conv7 = self.up_block(conv6,conv5,256)\n",
    "        conv8 = self.up_block(conv7,conv4,128)\n",
    "        conv9 = self.up_block(conv8,conv3,64)\n",
    "        conv10 = self.up_block(conv9,conv2,32)\n",
    "        conv11 = self.up_block(conv10,conv1,16)\n",
    "\n",
    "        outputs = tf.keras.layers.Conv2D(21, (1, 1), activation='sigmoid')(conv11)\n",
    "\n",
    "        self.model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "\n",
    "        self.model.compile(optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "                name='Adam'),\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                      metrics=['acc'])\n",
    "#         self.model.summary()\n",
    "        return self.model\n",
    "\n",
    "    def fit(self,train_dataset,steps_train,Epochs,val_dataset=None,steps_val=None):\n",
    "        self.model.fit_generator(train_dataset,validation_data=val_dataset,steps_per_epoch=steps_train,validation_steps = steps_val, epochs=Epochs)\n",
    "        \n",
    "    def predict(self,test_image):\n",
    "        x,y,_ = test_image.shape\n",
    "        test_image = np.array(test_image)\n",
    "        test_im = cv2.resize(test_image, (128,128), interpolation = cv2.INTER_NEAREST)\n",
    "        test_im = test_im / 255.0\n",
    "        test_im = cv2.GaussianBlur(test_im,(5,5),cv2.BORDER_DEFAULT)\n",
    "        trst_im = tf.convert_to_tensor(test_im)\n",
    "        test_im = tf.expand_dims(test_im, axis=0)\n",
    "        pr = self.model.predict(test_im)\n",
    "        pred_mask = tf.argmax(pr[0], axis=-1)\n",
    "        pred_mask = pred_mask[..., tf.newaxis]\n",
    "        pred_mask = cv2.resize(np.array(pred_mask), (y,x), interpolation = cv2.INTER_NEAREST)\n",
    "        return pred_mask\n",
    "    \n",
    "    def __call__(self):\n",
    "        return self.Model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T18:48:10.074483Z",
     "iopub.status.busy": "2022-05-18T18:48:10.074199Z",
     "iopub.status.idle": "2022-05-18T18:48:10.086728Z",
     "shell.execute_reply": "2022-05-18T18:48:10.086044Z",
     "shell.execute_reply.started": "2022-05-18T18:48:10.074449Z"
    }
   },
   "outputs": [],
   "source": [
    "# input_shape = (dim[0], dim[1], 3)\n",
    "# Unet =  Unet_model(input_shape)\n",
    "# model_U = Unet()\n",
    "# Unet.fit(train_batches,trainsplit_X.shape[0]//BATCH_SIZE, 100,val_batches,val_X.shape[0]//BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T18:48:10.088404Z",
     "iopub.status.busy": "2022-05-18T18:48:10.087966Z",
     "iopub.status.idle": "2022-05-18T18:48:10.095533Z",
     "shell.execute_reply": "2022-05-18T18:48:10.094726Z",
     "shell.execute_reply.started": "2022-05-18T18:48:10.088367Z"
    }
   },
   "outputs": [],
   "source": [
    "# P=Unet.predict(train_df[\"img\"][0])\n",
    "# fig, axs = plt.subplots(1,3)\n",
    "# axs[0].imshow(train_df[\"img\"][0])\n",
    "# axs[1].imshow(train_df[\"seg\"][0])\n",
    "# axs[2].imshow(P)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T18:48:10.097291Z",
     "iopub.status.busy": "2022-05-18T18:48:10.097016Z",
     "iopub.status.idle": "2022-05-18T18:48:10.10498Z",
     "shell.execute_reply": "2022-05-18T18:48:10.104161Z",
     "shell.execute_reply.started": "2022-05-18T18:48:10.097256Z"
    }
   },
   "outputs": [],
   "source": [
    "# for i in range(test_df[\"img\"].shape[0]):\n",
    "#     P=Unet.predict(test_df[\"img\"][i])\n",
    "#     test_df[\"seg\"][i]=P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T18:48:10.106567Z",
     "iopub.status.busy": "2022-05-18T18:48:10.106165Z",
     "iopub.status.idle": "2022-05-18T18:48:10.113854Z",
     "shell.execute_reply": "2022-05-18T18:48:10.113166Z",
     "shell.execute_reply.started": "2022-05-18T18:48:10.10653Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(test_df[\"seg\"][6].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T18:48:10.115403Z",
     "iopub.status.busy": "2022-05-18T18:48:10.115061Z",
     "iopub.status.idle": "2022-05-18T18:48:10.122904Z",
     "shell.execute_reply": "2022-05-18T18:48:10.122215Z",
     "shell.execute_reply.started": "2022-05-18T18:48:10.11536Z"
    }
   },
   "outputs": [],
   "source": [
    "# n = 101\n",
    "# plt.imshow(test_df[\"seg\"][n])\n",
    "# plt.show()\n",
    "# plt.imshow(test_df[\"img\"][n])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Transfer Learning\n",
    "The model from scratch isn't performing very well. We will see that a model based on a pretrained model will perform better. Different transfer learning model were implemented and tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-19T12:50:03.888251Z",
     "iopub.status.busy": "2022-05-19T12:50:03.887959Z",
     "iopub.status.idle": "2022-05-19T12:50:24.050032Z",
     "shell.execute_reply": "2022-05-19T12:50:24.049082Z",
     "shell.execute_reply.started": "2022-05-19T12:50:03.888217Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/tensorflow/examples.git\n",
    "from tensorflow_examples.models.pix2pix import pix2pix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For transfer learning we tested three different architectures to find the best one. We have compared MobilNetV2, DensNet and ResNet. MobilNetV2 was the first attempt to make the model work, it didn't perform very well and was not used further. In literature we found that ResNet perform well, it uses skip connections via addition. Finally, we tryed DensNet. DensNet outperformed the two previous tested architectures. DensNet uses dense connections between layers, that connects all layers with each other. This last architectures was fastest to train, it needed less epochs, and was more accurate than the two others. The same optimizer and loss is used for semantic segmentation with transfer learning, namely Adam optimizer and categorical crossentropy as loss function. Adam optimizer is a gradient descent method, this is an iterative method and perform very well, it is used in a lot of applications. Science we have a multi segmentation task to do, categorical crossentropy as loss function is an obvious choice. It allows using more than two labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-19T10:32:59.484297Z",
     "iopub.status.busy": "2022-05-19T10:32:59.483982Z",
     "iopub.status.idle": "2022-05-19T10:33:01.447142Z",
     "shell.execute_reply": "2022-05-19T10:33:01.446451Z",
     "shell.execute_reply.started": "2022-05-19T10:32:59.484264Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "def conv_block(input, num_filters):\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def decoder_block(input, skip_features, num_filters):\n",
    "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "def build_resnet50_unet(input_shape,output_channels):\n",
    "    \"\"\" Input \"\"\"\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    \"\"\" Pre-trained ResNet50 Model \"\"\"\n",
    "    resnet50 = ResNet50(include_top=False, weights=\"imagenet\", input_tensor=inputs)\n",
    "\n",
    "    \"\"\" Encoder \"\"\"\n",
    "    s1 = resnet50.get_layer(resnet50.layers[0].name).output           ## (512 x 512)\n",
    "    s2 = resnet50.get_layer(\"conv1_relu\").output        ## (256 x 256)\n",
    "    s3 = resnet50.get_layer(\"conv2_block3_out\").output  ## (128 x 128)\n",
    "    s4 = resnet50.get_layer(\"conv3_block4_out\").output  ## (64 x 64)\n",
    "\n",
    "    \"\"\" Bridge \"\"\"\n",
    "    b1 = resnet50.get_layer(\"conv4_block6_out\").output  ## (32 x 32)\n",
    "\n",
    "    \"\"\" Decoder \"\"\"\n",
    "    d1 = decoder_block(b1, s4, 512)                     ## (64 x 64)\n",
    "    d2 = decoder_block(d1, s3, 256)                     ## (128 x 128)\n",
    "    d3 = decoder_block(d2, s2, 128)                     ## (256 x 256)\n",
    "    d4 = decoder_block(d3, s1, 64)                      ## (512 x 512)\n",
    "\n",
    "    \"\"\" Output \"\"\"\n",
    "    outputs = Conv2D(output_channels, 1, padding=\"same\", activation=\"sigmoid\")(d4)\n",
    "\n",
    "    model = Model(inputs, outputs, name=\"ResNet50_U-Net\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-19T10:34:48.241313Z",
     "iopub.status.busy": "2022-05-19T10:34:48.241049Z",
     "iopub.status.idle": "2022-05-19T10:35:35.477134Z",
     "shell.execute_reply": "2022-05-19T10:35:35.476382Z",
     "shell.execute_reply.started": "2022-05-19T10:34:48.241284Z"
    }
   },
   "outputs": [],
   "source": [
    "class SemanticSegmentationTL:\n",
    "    \n",
    "    def __init__(self,model,input_shape,nb_channels):\n",
    "        self.model = model\n",
    "        self.input_shape = input_shape\n",
    "        self.nb_channels = nb_channels\n",
    "        \n",
    "    def unet_model(self):\n",
    "        inputs = tf.keras.layers.Input(shape=self.input_shape)\n",
    "        # Downsampling through the model\n",
    "        skips = self.down_stack(inputs)\n",
    "        x = skips[-1]\n",
    "        skips = reversed(skips[:-1])\n",
    "        # Upsampling and establishing the skip connections\n",
    "        for up, skip in zip(self.up_stack, skips):\n",
    "            x = up(x)\n",
    "            concat = tf.keras.layers.Concatenate()\n",
    "            x = concat([x, skip])\n",
    "        # This is the last layer of the model\n",
    "        last = tf.keras.layers.Conv2DTranspose(\n",
    "        filters=self.nb_channels, kernel_size=3, strides=2,\n",
    "            padding='same')  #64x64 -> 128x128\n",
    "\n",
    "        x = last(x)\n",
    "        return tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "        \n",
    "    def Model(self):\n",
    "        if self.model==\"MobileNetV2\":\n",
    "            base_model = tf.keras.applications.MobileNetV2(input_shape=self.input_shape, include_top=False)\n",
    "            # Use the activations of these layers\n",
    "            layer_names = [\n",
    "                'block_1_expand_relu',   # 64x64\n",
    "                'block_3_expand_relu',   # 32x32\n",
    "                'block_6_expand_relu',   # 16x16\n",
    "                'block_13_expand_relu',  # 8x8\n",
    "                'block_16_project',      # 4x4\n",
    "            ]\n",
    "            base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
    "\n",
    "            # Create the feature extraction model\n",
    "            self.down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\n",
    "\n",
    "            self.down_stack.trainable = False\n",
    "            self.up_stack = [\n",
    "                pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n",
    "                pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n",
    "                pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n",
    "                pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n",
    "            ]\n",
    "            self.modelTL = self.unet_model()\n",
    "            \n",
    "        elif self.model==\"Densnet\":\n",
    "            base_model = tf.keras.applications.DenseNet121(input_shape=self.input_shape, \n",
    "                                       include_top=False, weights='imagenet') \n",
    "            layer_names = ['conv1/relu', # size 64*64\n",
    "                          'pool2_relu',  # size 32*32\n",
    "                          'pool3_relu',  # size 16*16\n",
    "                          'pool4_relu',  # size 8*8\n",
    "                          'relu'        # size 4*4\n",
    "                          ] \n",
    "            base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
    "            self.down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\n",
    "\n",
    "            self.down_stack.trainable = False\n",
    "            self.up_stack = [\n",
    "                pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n",
    "                pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n",
    "                pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n",
    "                pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n",
    "            ]\n",
    "            self.modelTL = self.unet_model()\n",
    "        elif self.model==\"Resnet\":\n",
    "            self.modelTL = build_resnet50_unet(self.input_shape,self.nb_channels)\n",
    "            \n",
    "        self.modelTL.compile(loss= tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "             optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "                name='Adam'),\n",
    "             metrics=['accuracy']) \n",
    "        return self.modelTL\n",
    "            \n",
    "    def fit(self,train_dataset,steps_train,Epochs,val_dataset=None,steps_val=None):\n",
    "        hist = self.modelTL.fit_generator(train_dataset, epochs=Epochs,validation_data=val_dataset,steps_per_epoch=steps_train,validation_steps = steps_val)\n",
    "        return hist\n",
    "    \n",
    "    def  predict(self,test_image):\n",
    "        x,y,_ = test_image.shape\n",
    "        test_image = np.array(test_image)\n",
    "        test_im = cv2.resize(test_image, (128,128), interpolation = cv2.INTER_NEAREST)\n",
    "        test_im = test_im / 255.0\n",
    "        test_im = cv2.GaussianBlur(test_im,(5,5),cv2.BORDER_DEFAULT)\n",
    "        trst_im = tf.convert_to_tensor(test_im)\n",
    "        test_im = tf.expand_dims(test_im, axis=0)\n",
    "        pr = self.modelTL.predict(test_im)\n",
    "        pred_mask = tf.argmax(pr[0], axis=-1)\n",
    "        pred_mask = pred_mask[..., tf.newaxis]\n",
    "        pred_mask = cv2.resize(np.array(pred_mask), (y,x), interpolation = cv2.INTER_NEAREST)\n",
    "        return pred_mask\n",
    "            \n",
    "    def __call__(self):\n",
    "        return self.Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-19T10:35:42.022321Z",
     "iopub.status.busy": "2022-05-19T10:35:42.021593Z",
     "iopub.status.idle": "2022-05-19T10:35:42.203168Z",
     "shell.execute_reply": "2022-05-19T10:35:42.202453Z",
     "shell.execute_reply.started": "2022-05-19T10:35:42.022282Z"
    }
   },
   "outputs": [],
   "source": [
    "input_shape = (dim[0], dim[1], 3)\n",
    "TLSS =  SemanticSegmentationTL(\"Densnet\",input_shape,21)\n",
    "\n",
    "MODELSS = TLSS()\n",
    "MODELSS.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-19T10:36:03.789748Z",
     "iopub.status.busy": "2022-05-19T10:36:03.789433Z",
     "iopub.status.idle": "2022-05-19T10:36:04.548342Z",
     "shell.execute_reply": "2022-05-19T10:36:04.547401Z",
     "shell.execute_reply.started": "2022-05-19T10:36:03.789714Z"
    }
   },
   "outputs": [],
   "source": [
    "hist = TLSS.fit(train_batches,trainsplit_X.shape[0]//BATCH_SIZE, 30,val_batches,val_X.shape[0]//BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T18:48:29.974421Z",
     "iopub.status.busy": "2022-05-18T18:48:29.973783Z",
     "iopub.status.idle": "2022-05-18T18:48:29.98279Z",
     "shell.execute_reply": "2022-05-18T18:48:29.981853Z",
     "shell.execute_reply.started": "2022-05-18T18:48:29.974381Z"
    }
   },
   "outputs": [],
   "source": [
    "loss = hist.history['loss']\n",
    "val_loss = hist.history['val_loss']\n",
    "\n",
    "acc = hist.history['accuracy']\n",
    "val_acc = hist.history['val_accuracy']\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1,2)\n",
    "axs[0].plot(hist.epoch, loss, 'r', label='Training loss')\n",
    "axs[0].plot(hist.epoch, val_loss, 'bo', label='Validation loss')\n",
    "axs[0].set_title('Training and Validation Loss')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].set_ylabel('Loss Value')\n",
    "axs[0].set_ylim([0, 1])\n",
    "axs[0].legend()\n",
    "axs[1].plot(hist.epoch, acc, 'r', label='Training accuracy')\n",
    "axs[1].plot(hist.epoch, val_acc, 'bo', label='Validation accuracy')\n",
    "axs[1].set_title('Training and Validation Accuracy')\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].set_ylabel('Accuracy')\n",
    "axs[1].set_ylim([0, 1])\n",
    "axs[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained and now we can predict the segmentations for the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T18:48:29.986157Z",
     "iopub.status.busy": "2022-05-18T18:48:29.984668Z",
     "iopub.status.idle": "2022-05-18T18:48:29.994328Z",
     "shell.execute_reply": "2022-05-18T18:48:29.993512Z",
     "shell.execute_reply.started": "2022-05-18T18:48:29.986115Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(test_df[\"img\"].shape[0]):\n",
    "    P=TLSS.predict(test_df[\"img\"][i])\n",
    "    test_df[\"seg\"][i]=P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction for the training data looks very similar to the real segmentation image. The example shown here, can easily reconstruct the bird in the predicted segmentation image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=460\n",
    "P=TLSS.predict(train_df[\"img\"][n])\n",
    "fig, axs = plt.subplots(1,3)\n",
    "axs[0].imshow(train_df[\"img\"][n])\n",
    "axs[1].imshow(train_df[\"seg\"][n])\n",
    "axs[2].imshow(P)\n",
    "plt.show()\n",
    "print('nb of pixels labeld as bird in real segmentation: '+str(train_df[\"seg\"][n][train_df[\"seg\"][n]==labels.get_loc(\"bird\")+1].shape))\n",
    "print('nb of pixels labeld as bird in prediction: '+str(P[P==labels.get_loc(\"bird\")+1].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction from the validation set is very different, some images can be segmented easily, others have difficulties e.g. noisy images are segmented difficultly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-19T12:50:24.052832Z",
     "iopub.status.busy": "2022-05-19T12:50:24.052489Z",
     "iopub.status.idle": "2022-05-19T12:50:26.692697Z",
     "shell.execute_reply": "2022-05-19T12:50:26.691907Z",
     "shell.execute_reply.started": "2022-05-19T12:50:24.052785Z"
    }
   },
   "outputs": [],
   "source": [
    "for image, mask in val_batches.take(1):\n",
    "    pred_mask = MODELSS.predict(image)\n",
    "    pred_mask = tf.argmax(pred_mask[0], axis=-1)\n",
    "    pred_mask = pred_mask[..., tf.newaxis]\n",
    "    fig, axs = plt.subplots(1,3)\n",
    "    axs[0].imshow(image[0])\n",
    "    axs[1].imshow(mask[0])\n",
    "    axs[2].imshow(pred_mask)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om the image below we see a bird. This image is segmented pretty well. Images with few different object is segmented accuratly enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-19T13:00:09.172604Z",
     "iopub.status.busy": "2022-05-19T13:00:09.171861Z",
     "iopub.status.idle": "2022-05-19T13:00:10.108000Z",
     "shell.execute_reply": "2022-05-19T13:00:10.107201Z",
     "shell.execute_reply.started": "2022-05-19T13:00:09.172564Z"
    }
   },
   "outputs": [],
   "source": [
    "n=22\n",
    "P=TLSS.predict(test_df[\"img\"][n])\n",
    "fig, axs = plt.subplots(1,2)\n",
    "axs[0].imshow(test_df[\"img\"][n])\n",
    "axs[1].imshow(test_df[\"seg\"][n])\n",
    "plt.show()\n",
    "print('nb of pixels labeld as bird: '+str(test_df[\"seg\"][n][test_df[\"seg\"][n]==labels.get_loc(\"bird\")+1].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the previous image, this image contains a lot of different objects. In the segmentation image we see different objects in different location, it recongnize the class of the object well, but has problems to find the full/correct location of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=62\n",
    "P=TLSS.predict(test_df[\"img\"][n])\n",
    "fig, axs = plt.subplots(1,2)\n",
    "axs[0].imshow(test_df[\"img\"][n])\n",
    "axs[1].imshow(test_df[\"seg\"][n])\n",
    "plt.show()\n",
    "print('nb of pixels labeld as tvmonitor: '+str(test_df[\"seg\"][n][test_df[\"seg\"][n]==labels.get_loc(\"tvmonitor\")+1].shape))\n",
    "print('nb of pixels labeld as person: '+ str(test_df[\"seg\"][n][test_df[\"seg\"][n]==labels.get_loc(\"person\")+1].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model only 30 epochs are used, increasing this value should ensure a beter performance. Having more time, we could further experiment with other architectures and more paramters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.196901,
     "end_time": "2022-04-12T14:49:19.951018",
     "exception": false,
     "start_time": "2022-04-12T14:49:19.754117",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submit to competition\n",
    "You don't need to edit this section. Just use it at the right position in the notebook. See the definition of this function in Sect. 1.3 for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-19T13:06:43.294741Z",
     "iopub.status.busy": "2022-05-19T13:06:43.294455Z",
     "iopub.status.idle": "2022-05-19T13:06:46.167479Z",
     "shell.execute_reply": "2022-05-19T13:06:46.166751Z",
     "shell.execute_reply.started": "2022-05-19T13:06:43.294710Z"
    }
   },
   "outputs": [],
   "source": [
    "# export solutions to good format\n",
    "test_df.loc[:, labels] = yhat\n",
    "test_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-19T13:06:54.145802Z",
     "iopub.status.busy": "2022-05-19T13:06:54.145099Z",
     "iopub.status.idle": "2022-05-19T13:07:03.080816Z",
     "shell.execute_reply": "2022-05-19T13:07:03.079914Z",
     "shell.execute_reply.started": "2022-05-19T13:06:54.145763Z"
    },
    "papermill": {
     "duration": 81.176133,
     "end_time": "2022-04-12T14:50:41.324425",
     "exception": false,
     "start_time": "2022-04-12T14:49:20.148292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_submission(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.197466,
     "end_time": "2022-04-12T14:50:41.721228",
     "exception": false,
     "start_time": "2022-04-12T14:50:41.523762",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Adversarial attack\n",
    "For this part, we try to fool the classification and CNN, using an *adversarial attack*. More specifically, the goal is build a CNN to perturb test images in a way that (i) they look unperturbed to humans; but (ii) the CNN classifies these images in line with the perturbations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast Gradient Sign Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fast gradient sign method is a kind of adversial attack based on the white-box approach. It is called like this because it uses the sign of the gradient to add noice in the same direction as the gradient to the image. The noice is scaled by epsilon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:32:50.138608Z",
     "iopub.status.busy": "2022-05-20T15:32:50.138317Z",
     "iopub.status.idle": "2022-05-20T15:32:50.143572Z",
     "shell.execute_reply": "2022-05-20T15:32:50.142354Z",
     "shell.execute_reply.started": "2022-05-20T15:32:50.138577Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below, constructs the gradient signs for the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:33:03.019782Z",
     "iopub.status.busy": "2022-05-20T15:33:03.019416Z",
     "iopub.status.idle": "2022-05-20T15:33:03.028273Z",
     "shell.execute_reply": "2022-05-20T15:33:03.026150Z",
     "shell.execute_reply.started": "2022-05-20T15:33:03.019750Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_image_adversary(model, image, label):\n",
    "    # cast the image\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    # record our gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        # explicitly indicate that our image should be tacked for gradient updates\n",
    "        tape.watch(image)\n",
    "        # use our model to make predictions on the input image and then compute the loss   \n",
    "        pred = model(tf.reshape(image, (1,128,128,3)))\n",
    "        loss = categorical_crossentropy(label, pred)\n",
    "    # calculate the gradients of loss with respect to the image, then compute the sign of the gradient\n",
    "    gradient = tape.gradient(loss, image)\n",
    "    signedGrad = tf.sign(gradient)\n",
    "    # construct the image adversary\n",
    "    return signedGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below an example for the attack is given. First an image is shown and then the CNN for the classification with transfer learning is used for predicting the labels for this image. As can be seen, the CNN gives a good result for this image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:34:26.208729Z",
     "iopub.status.busy": "2022-05-20T15:34:26.208399Z",
     "iopub.status.idle": "2022-05-20T15:34:26.463970Z",
     "shell.execute_reply": "2022-05-20T15:34:26.463062Z",
     "shell.execute_reply.started": "2022-05-20T15:34:26.208700Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(train_df[\"img\"][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:34:39.460048Z",
     "iopub.status.busy": "2022-05-20T15:34:39.459739Z",
     "iopub.status.idle": "2022-05-20T15:34:39.544473Z",
     "shell.execute_reply": "2022-05-20T15:34:39.542716Z",
     "shell.execute_reply.started": "2022-05-20T15:34:39.460016Z"
    }
   },
   "outputs": [],
   "source": [
    "testImage = train_x[4,:,:,:]\n",
    "pred = transferModel.predict(testImage.reshape(1,128,128,3))\n",
    "top = np.argsort(pred[0])[:-4:-1]\n",
    "for i in range(len(top)):\n",
    "    index = top[i]\n",
    "    print(train_df.columns[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now an epsilon is chosen. The higher the epsilon, the bigger the noise is. When the epsilon is too high, it would be visible for the naked eye, however when choosing a moderate epsilon it is not visible. Then the signs of the gradient for this image combined with the CNN is calculated using the function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:36:37.850982Z",
     "iopub.status.busy": "2022-05-20T15:36:37.850571Z",
     "iopub.status.idle": "2022-05-20T15:36:38.384464Z",
     "shell.execute_reply": "2022-05-20T15:36:38.383444Z",
     "shell.execute_reply.started": "2022-05-20T15:36:37.850934Z"
    }
   },
   "outputs": [],
   "source": [
    "eps = 0.5\n",
    "signedGrad = generate_image_adversary(transferModel, testImage, tf.constant(train_yC[4], shape=(1,20)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising the signs of the gradient reuslts in a black image. This is because the gradient results in all zeros. This means there will be no noise added to the image and the adversial attack will not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:37:01.606029Z",
     "iopub.status.busy": "2022-05-20T15:37:01.605493Z",
     "iopub.status.idle": "2022-05-20T15:37:01.835285Z",
     "shell.execute_reply": "2022-05-20T15:37:01.834071Z",
     "shell.execute_reply.started": "2022-05-20T15:37:01.605992Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(signedGrad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the adversial image looks a bit different, however this is because this image is preprocessed. However it looks the same as the preprocessed image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:38:34.344436Z",
     "iopub.status.busy": "2022-05-20T15:38:34.344140Z",
     "iopub.status.idle": "2022-05-20T15:38:34.603978Z",
     "shell.execute_reply": "2022-05-20T15:38:34.603016Z",
     "shell.execute_reply.started": "2022-05-20T15:38:34.344405Z"
    }
   },
   "outputs": [],
   "source": [
    "adversary = (testImage + (signedGrad * eps)).numpy()\n",
    "plt.imshow(adversary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:39:51.926652Z",
     "iopub.status.busy": "2022-05-20T15:39:51.926362Z",
     "iopub.status.idle": "2022-05-20T15:39:52.008450Z",
     "shell.execute_reply": "2022-05-20T15:39:52.007266Z",
     "shell.execute_reply.started": "2022-05-20T15:39:51.926620Z"
    }
   },
   "outputs": [],
   "source": [
    "predAdv = transferModel.predict(tf.reshape(adversary, (1,128,128,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said above, since the signedGrad contained only zeros, the image will not have changed and the CNN can predict the lables for this image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-20T15:41:13.031029Z",
     "iopub.status.busy": "2022-05-20T15:41:13.030553Z",
     "iopub.status.idle": "2022-05-20T15:41:13.042564Z",
     "shell.execute_reply": "2022-05-20T15:41:13.040920Z",
     "shell.execute_reply.started": "2022-05-20T15:41:13.030982Z"
    }
   },
   "outputs": [],
   "source": [
    "top = np.argsort(predAdv[0])[:-4:-1]\n",
    "for i in range(len(top)):\n",
    "    index = top[i]\n",
    "    print(train_df.columns[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not sure why the gradient of the image only contains zeros. \n",
    "\n",
    "This kind of attack is a white box attack, this means the attacker must have full access to the model. Therefore in the real world, it is difficult to attack a system using this method. However, there also exist black-box attacks which can attack a model without knowlegde of the model. Therefore, the model needs some defense strategies to be more reliable for white box as well as black box attacks. A strategy to improve the robustness is the 'Adversial training'. It is a technique where the neural network is trained on adversial images. An adversial image can be constructed by using the fast gradient sign method as shown above. However, there are some other techniques. In short these techniques are: PGD adversial training, Ensemble adversarial training, Adversarial logit pairing and Generative adversarial training. Another strategy is 'Randomization'. A randomization-based defense attempts to randomize the adversarial effects into random effects, this is done by adding some extra layers in front of the neural network or in between. There are different methods which fall under this category, an example is 'Random input transformation'. In this technique the image, which is fed to the neural network, is first resized to a random size and then the image is randomly padded. Padding is adding zeros around the image. Then this image is fed to the neural network and the model can start to train. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.195695,
     "end_time": "2022-04-12T14:50:42.117581",
     "exception": false,
     "start_time": "2022-04-12T14:50:41.921886",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Discussion\n",
    "Finally, take some time to reflect on what you have learned during this assignment. Reflect and produce an overall discussion with links to the lectures and \"real world\" computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lectures gave a good introduction to different methods and gave some theoretical background of it. In the assignment some methods were tested on a application. On of the points that we saw is: despite of having a good theoretical model it will not work if it hasn't had a good preprocessing on the input.\n",
    "\n",
    "The classification and segmentation models worked decently for images that were very clear with the object and the boundaries of the object. With transfer learning model we have good results for both.\n",
    "When there is an overlap the output is not great. For some images where classification works correctly we have proper implementation of the adversary attack.\n",
    "\n",
    "In data augmentation we must note that rotating the images worsens the segmentation and classification resukts but flipping it is good enough.\n",
    "\n",
    "Some more time could be spent in modifying the model for segmentation and classification for the ones we have to make from scratch.\n",
    "Our models can only be applied to cases where the images are very clear with proper distinction of background and object.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
